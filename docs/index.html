---
layout: default
title: CompReSS 
---

<div style="height:25px;">
<p style="text-align:center;"><a href="https://www.csee.umbc.edu/~soroush">Soroush Abbasi Koohpayegani</a><sup>∗</sup>, <a href="">Ajinkya Tejankar</a><sup>∗</sup> , <a href="">Vipin Pillai</a>, <a href="https://www.csee.umbc.edu/~hpirsiav/">Hamed Pirsiavash</a></p>
</div>
<div style="height:25px;">
<p style="text-align:center;">University of Maryland, Baltimore County</p>
</div>
<div style="height:30px;">
<p style="text-align:center; font-size:12px"><sup>∗</sup> denote equal contribution</p>
</div>
<div style="height:30px;">
<br>

<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/teaser.png" width="750" alt style></p>

<h5 id="abstract"><b>Abstractt</b></h5>
<p>Self-supervised learning aims to learn good representations with unlabeled data. Recent works have shown that larger models benefit more from self-supervised learning than
    smaller models. As a result, the gap between the supervised and self-supervised learning has been greatly reduced for larger models. In this work, we focus on 
    self-supervised learning for low capacity models that has various applications (e.g., edge computation). We compress a deep teacher model so that the student mimics the
    relative distances between the datapoints in the teacher embeddings. For ResNet-50, our method outperforms SOTA self-supervised models marginally on ImageNet linear 
    evaluation and with a large margin on nearest neighbor evaluation (by 6 points). For AlexNet, our method outperforms all previous methods including the fully supervised
    model on ImageNet linear evaluation (57.6% compared to 56.5%) and by a large margin on nearest neighbor evaluation (52.3% compared to 41.4%). 
    This is the first time a self-supervised AlexNet has outperformed supervised one on ImageNet classification.</p>

<h5 id="paper"><b>Paper</b></h5>
<p><a href="https://arxiv.org/">CompReSS: Compressing Representations for Self-Supervised Learning</a></p>
<h5 id="code"><b>Code</b></h5>
<p><a href="https://github.com/UMBCvision/CompReSS">Official CompReSS code</a></p>

<h5 id="contributions"><b>Contributions</b></h5>


<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/compare_graph.png" width="750" alt style></p>

<h5 id="results"><b>Results</b></h5>


<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table1.png" width="750" alt style></p>
    <p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table2.png" width="750" alt style></p>
    <p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/result_table3.png" width="750" alt style></p>



<h5 id="Abilation Study"><b>Abilation Study</b></h5>
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/abilative.png" width="750" alt style></p>
    
<h5 id="cluster alignment"><b>Cluster Alignment Result</b></h5>   
<p style="text-align:center;"><img src="{{ site.baseurl }}/assets/images/query_img_large.jpg" width="750" alt style></p>

<h5 id="references"><b>References</b></h5>
[1] 
